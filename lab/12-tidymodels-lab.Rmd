---
title: "Modelling with `tidymodels`"
subtitle: "Practice Materials"
author: "Franka Tetteroo, Linus Hagemann & Sofiya Berdiyeva"
date: "28.10.2024"
output: 
    rmdformats::robobook:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    toc_depth: 3
    toc_float: true
    self_contained: false
---

```{=html}
<style>
.h1,h2,h3 {
color:#2f1a61;
}

.subtitle, section.normal {
color:#291854;
}

.title {
color:#cc0065;
}

.nav-pills>li>a{
color: #2f1a61;
}

.nav-pills>li.active>a, .nav-pills>li.active>a:hover, .nav-pills>li.active>a:focus {
color: #fff;
background-color: #2f1a61;
}

.nav-tabs>li>a{
color: #2f1a61;
}

.nav-tabs>li.active>a, .nav-tabs>li.active>a:hover, .nav-tabs>li.active>a:focus {
color: #fff;
background-color: #2f1a61;
}

div.danger {background-color:#c42d2d; color:white; border-radius: 5px; padding: 20px;}
div.further {background-color:#7CB9E8; color:white; border-radius: 5px; padding: 20px;}
div.info {background-color:#66FF99; color:black; border-radius: 5px; padding: 20px;}

</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

This session is designed to guide you through the entire modelling process to familiarize yourself with some of the power of the packages in `tidymodels`. 

We will cover:

- splitting your data-set into a training and a test set using `rsample`,
- pre-processing your data-set using `recipes`,
- fitting a model using `parsnip`,
- and finally evaluate the performance of the model using `yardstick`.

For those of you attending this session live - stay tuned, as __there will be a price to be won in the end__!

![](./attendance_price.jpg)

::: danger
‚ö†Ô∏è __Attention__

Modelling is an extremely complex topic. Choosing an appropriate model for a specific task requires theoretical deliberation and careful consideration of the data at hand. As was seen in the lecture, hand-picking specific model configurations can rightfully be criticized and requires justification grounded in theory.

All of these are outside of the scope of this workshop - we aim at giving you a glimpse on how to use selected `R` packages. Doing so in a very condensed fashion requires us to take some shortcuts and trade-offs. Please be mindful of this when consulting this material for your future modelling adventures - there's a lot of more to think about!
:::

# Our Dataset

Throughout this exercise, we are working with a specially prepared version of the [`ames`](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt) data-set, which is part of the `modeldata` package. The data-set contains information about houses in the region of Ames in Iowa (US). We prepared a smaller version of the data-set for this unit, in order to reduce the amount of different features (columns) to understand in the data-set and introduce some need for pre-processing.

```{r}
# install.packages("tidymodels")
library(tidymodels)
set.seed(42)

load("ames_workshop.RData") # executing this line will lead to the `ames_workshop` df becoming available in your environment
ames_workshop
```

As we can see, there are close to 3000 houses in the data-set, with 16 data points for each house.

The following table gives you a quick overview of the available features (in the order in which the columns appear in the data frame). More detailed information can be found [here](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt).

| Name         | Description                                                       |
|--------------|-------------------------------------------------------------------|
| MS_Zoning    | Identifies the general zoning classification of the sale.         |
| Lot_Frontage | Linear feet of street connected to property                       |
| Lot_Area     | Lot size in square feet                                           |
| Utilities    | Type of utilities available                                       |
| Neighborhood | Physical locations within Ames city limits (map available)        |
| Condition_1  | Proximity to various conditions                                   |
| Condition_2  | Proximity to various conditions (if more than one is present)     |
| Overall_Cond | Rates the overall condition of the house                          |
| Exter_Cond   | Evaluates the present condition of the material on the exterior   |
| Foundation   | Type of foundation                                                |
| Heating      | Type of heating                                                   |
| Year_Sold    | Year Sold (YYYY)                                                  |
| Garage_Area  | Size of garage in square feet                                     |
| Mo_Sold      | Month Sold (MM)                                                   |
| Sale_Price   | Sale price \$$                                                    |
| secret_sauce | A special feature just for you! Can this improve our predictions? |

Our goal in this session is to predict the `Sale_Price` of a house based on the other available data-points. For this, we will build a linear model. You can already start to think about which features you think would be most useful for this task! 

# The Modelling Process with `tidymodels`

## Feature Egineering

First, we need to pre-process our data in order to make it suitable for use inside of a model. Remember, that each type of model has some requirements on the data, which are grounded in theory. __We will not focus too much on this here. This means this guide is really just concerned with how to do the modelling process with `tidymodels`. Thus, the modelling itself is not extremely solid.__

Let's investigate the data-set a bit:

```{r}
head(ames_workshop)
```
From this alone, we can already gather some aspects of the data-set which are problematic. In the `Lot_Frontage` column we can see missing values. Additionally, we can already see that `Utilities` are contained as strings. These we cannot use in our linear model and would like the column to be a [`factor`](https://r4ds.had.co.nz/factors.html) instead. Let's use `recipe` in order to pre-process our data:

```{r}
rec <- recipe(ames_workshop) %>%
  update_role(c(MS_Zoning, Lot_Area, Overall_Cond, Exter_Cond, Year_Sold), 
              new_role = "predictor") %>%
  update_role(Sale_Price, new_role = "outcome") %>%
  step_impute_mean(all_numeric()) %>%
  step_string2factor(Utilities, levels = c("AllPub", "NoSewr", "NoSeWa")) %>%
  step_corr(all_numeric_predictors())
preped_recipe <- prep(rec, ames_workshop)
```

What's going on here?

1. We create a recipe concerned with the `ames_workshop` data-set, and will store the end-result in a variable called `rec`
2. Inside of recipes, variables can have different [roles](https://recipes.tidymodels.org/articles/Roles.html). The most important ones are "outcome" and "predictor". These specify which variables are our independent ones inside of the model and what is the outcome that we want to predict. We could also specify a `formula` already at this time, but postpone this for later.
3. After setting the appropriate roles, a recipe is a series of "steps" (see where the cooking analogy comes from? üßë‚Äçüç≥). Each step is named after a common task when doing feature-engineering/pre-processing. You can find all available steps in [the documentation](https://recipes.tidymodels.org/reference/index.html). As the argument to each step, we need to specify the variables on which to perform the step (the ingredients! üòâ). Here, the roles come in handy: `recipes` provides us with a set of functions which can be used to easily select the variables for a step, without the need to specify concrete variable names - this makes for easy code-reuse throughout different projects and make the code a bit more resistant towards name changes. We use the following steps in our recipe:

- Impute any missing values in any __numeric predictors__ with the __mean__ value of that column.
- Convert the strings in `Utilities` to a factors so that we can use it in our linear model.
- `step_corr` removes all numeric predictors which have high correlation between each other. For each group of variables that are highly correlated with each other, one will be kept.

4. `prep` now takes these instructions and prepares them on our data-set. This for instance includes finding the correlating predictors, calculating the necessary mean values for imputation and so on.

::: danger
At this point, we have not yet changed the original data-set, but just made the necessary preparations for that! Think of a bowl of dough - that's not a üéÇ yet!
:::

```{r}
ames_workshop <- bake(preped_recipe, ames_workshop)
```

`bake` is the final step in applying our recipe to our actual data-set. Usually, it would be good idea to __not overwrite__ your original data-set, so that you have an easier time to compare the data before and after pre-processing. We opted to this here for the sake of clearness.

Now, let's roughly check how the data looks now:

```{r}
head(ames_workshop)
```
As we can see, the missing values in `Lot_Frontage` are gone and the `Utilities` column is now coded as a `factor`.
Note also that we still have 16 variables - our predictors do not contain any highly correlated numerical columns! Remember - they would have been dropped otherwise by `step_corr`. 

## Sampling & bootstrapping

### Initial Split

We first split the entire dataset into the training and testing dataset (see slide 8). For a split of 75-25% we use .75 as argument.

```{r}
ames_split <- rsample::initial_split(ames_workshop, prop = .75)
ames_split
```
We can see it partitioned the datset into training (2197 observations) and testing (733 observations). It also gives us the total amount, namely 2930.

### Training and testing

We need one more step to have our training and testing dataset:  

```{r}
ames_train <- rsample::training(ames_split)
ames_test <- rsample::testing(ames_split)
```

Now we're ready to do some advanced splitting of our training dataset!


### Cross validation and V-Fold

This example will focus on v-fold cross validation to further split our training set. Basically, we tell R to split our dataset x amount of times in two groups with a set size (without replacement).

```{r}
ames_folds <- rsample::vfold_cv(ames_train, v = 5)
ames_folds

## to inspect the first fold we do the following:
#to see the amount of analysis vs assessment observations of the first fold:
ames_folds$splits[[1]] %>% analysis() %>% dim()
#to look into the analysis set of the first split:
ames_folds$splits[[1]] %>% analysis()
```

As you can see, we have now split the training set into 5 sets that consist of an analysis and assessment set. The analysis sets are used to run one or multiple models on, the assessment set acts as a test set to see how well the models explain new data. During research, we can use these various analysis and assessment sets to compare different models and see which one we want to run on our test set. Since we can only run one model on the test set, we need to be confident in the performance of our final model.

::: danger
‚ö†Ô∏è Important: The models need to be run on the analysis sets of the ` ames_fold`s tibble :) 
:::

## Fitting a Model

With our pre-processing done and equipped with multiple test and training-sets, we can finally get down to business and fit our model!
Remember, that our goal here is to predict `Sale_Price` using a linear regression model.

Although we did not specify a concrete formula in the `recipe` above, the `recipe` already contained a lot of assumptions about how our model should look like. This is, because we already set the "predictor" and "outcome" roles, which determine on which side of our model's formula variables will occur (and which ones won't make an appearance at this time).

As there are three rules for real estate valuation (location, location, and you guessed it, location), we decide to include the zoning in our model, which tells us whether a given house is inside of a residential area, close to industry plants, etc.
Additionally, we will include the overall and exterior condition of the house and the overall size of the property. These all seem important for the valuation of a house. As we have sales from different years in our data-set, we'll also include the year of the sale. The general idea behind this is to reflect general swings in the real estate markets and economic developments like inflation.

We use `parsnip` to denote our model in a headache-free, standardized way:

```{r}
model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(Sale_Price ~ MS_Zoning + Lot_Area + Overall_Cond + Exter_Cond + Year_Sold, 
      data = ames_folds$splits[[1]] %>% analysis())
```

Due to the power of `parsnip`, we can simply specify that we want to use a linear regression model, specifically the implementation provided by `stat::lm()`. The different implementations are called "engines" by `parsnip`.
Note, that we __use the training-set of our first split to train/fit the model!__.

## Model evaluation

Now, let's see how well our models perform:

:::danger 
‚ö† To evaluate model performance, always use the __test__ set of the data-split!
:::

```{r}
# parsnip model ALWAYS returns the results as a data frame
test_set <- ames_folds$splits[[1]] %>% assessment()

predictions <- bind_cols(
  select(test_set, Sale_Price),
  predict(model, test_set)
)
```

### And here comes yardstick

When "truth" is a factor (by classification), `metrics()` returns accuracy and the Kappa.
When "truth" is numeric (regression task), we get RMSE, R squared and MAE by default.

```{r}
# Estimate one or more common performance estimates depending on the class of truth 
metrics(predictions, truth = Sale_Price, estimate = .pred)

metrics_default <- metrics(predictions, truth = Sale_Price, estimate = .pred)
# Setting the number of digits for metrics
options(scipen = 999, digits=2)  # Set the number of digits to display
print(metrics_default)
```

To interpret errors, it would be helpful to have a look at the summary statistics of 'Sale Price'.

```{r}
summary(ames_workshop$Sale_Price)

#Standard deviation
sd(ames_workshop$Sale_Price)
```

As we can see, magnitude of both Root Mean Squared Error and Mean Absolute Error is pretty high (RMSE is almost equal to one standard deviation), which indicates a pretty low quality and usefulness of the model.


::: danger
‚ö†Ô∏è __Attention__

Yardstick has two functions to compute R squared:

1. `rsq()` - simply the squared correlation between truth and estimate.

2. `rsq_trad()` - coefficient of determination using the traditional definition with the sum of squares in the formula. 

The `rsq()` is the one specified as default in `metrics()`
:::

As we can see, predicted and real values have a weak correlation.

### Defining a set of regression metrics

```{r}
reg_metrics <- metric_set(ccc, iic)

# The returned function has the same arguments as metrics()
reg_metrics(predictions, truth = Sale_Price, estimate = .pred)
```

`ccc()` is a function for Concordance Correlation Coefficient, whose highest possible value (=1) indicates perfect strong concordance between predicted and true values, and its opponent (= -1) refers to strong discordance. Here what we observe is a weak concordance.

`iic` stands for Index of Ideality of Correlation. It shows us a 'predictive potential of a model' [(Toropova & Toropov, 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0048969717302279) and is seen as an alternative to the traditional correlation coefficient, so should be interpreted similarly ("low" in our case).

### Single metric

```{r}
mase(predictions, truth = Sale_Price, estimate = .pred)
```

Because we had a sort of temporal component in our data, we could be interested to assess the effectiveness of the forecasting. This could be done through Mean Absolute Scaled Error (MASE), a measure for determining the effectiveness of forecasts generated by model by comparing the predictions with the output of a na√Øve forecasting approach. [link](https://medium.com/@ashishdce/mean-absolute-scaled-error-mase-in-forecasting-8f3aecc21968)

Its value greater than one (1) indicates the algorithm is performing poorly compared to the na√Øve forecast. In our case, the value is not that dramatically high, but still leaves us with a slight anxiety and uncertainty.

---

# Exercise

::: info 
Remember - the goal of this exercise is to build a well-performing model predicting the `Sale_Price` of a house.
:::

## Loading and Tranforming the Data with `recipe`

```{r}
# Let's start from a clean slate and load the data-set anew!
load("ames_workshop.RData")
rec <- recipe(ames_workshop) %>%
  update_role(c(
    # YOUR PREDICTORS HERE
    # !!! SOLUTION JUST TO RUN THE CODE !!!
    MS_Zoning, Lot_Area, Overall_Cond, Exter_Cond, Year_Sold
  ), new_role = "predictor") %>%
  update_role(Sale_Price, new_role = "outcome") %>%
  # Feel free to play around with these and/or add new steps!
  # We recommend you this set of "starter-steps" as those will at least take care
  # of the most glaring problems in the data-set (that we introduced üôÉ).
  step_impute_mean(all_numeric()) %>%
  step_string2factor(Utilities, levels = c("AllPub", "NoSewr", "NoSeWa")) %>%
  step_corr(all_numeric_predictors())
preped_recipe <- prep(rec, ames_workshop)
# ames_workshop_baked <- # YOUR CODE TO APPLY THE PREPROCESSING HERE 
# !!! SOLUTION !!!
ames_workshop_baked <- bake(preped_recipe, ames_workshop)
```

## Sampling & Bootstrapping

### Initial Split

- Split the transformed data-set into a training and testing set.
- Use a proportion of .8.
- Extract the training and testing dataset from the initial split set. 

```{r}
# YOUR CODE HERE

#answer: 
ames_split_ex <- rsample::initial_split(ames_workshop_baked, prop = .8)
ames_split_ex

ames_train_ex <- rsample::training(ames_split_ex)
ames_test_ex <- rsample::testing(ames_split_ex)

```

### Bootstrapping

::: info
Bootstrapping is the second method we can use to split our training set. This time we sample from the training set __with replacement__ until we get a set including the number of rows as the original data set. The unused observations are then put in the assessment set. For more info, see slide 10. 
:::

- Use the `rsample::bootstraps` function to split the data-set __3__ times. 

::: info
Note: where `vfold_cv` uses `v = x√¨`, bootstraps employs `times = x`.
:::

- Extract the analysis sets.

```{r}
# This gives us a tibble with two columns: column 1 contains lists:
# Each list then contains an analysis and assessment set. The second column gives us the bootstrap ID.
# These become more important when working with a large number of bootstrap samples.
ames_bootstrap_ex <- rsample::bootstraps(ames_train_ex, times = 3)
ames_bootstrap_ex

# This part is not strictly necessary in order to proceed. Make sure to think about at some time, though. üôÇ
# Inspect the first bootstrap sample. How many observations do you expect in the analysis set?
ames_bootstrap_analysis <- # YOUR CODE HERE
ames_bootstrap_assessment <- # YOUR CODE HERE
# !!! SOLUTION !!! 
ames_bootstrap_analysis <- ames_bootstrap_ex$splits[[1]] %>% analysis()
ames_bootstrap_assessment <- ames_bootstrap_ex$splits[[1]] %>% assessment()
```

::: info
__Note:__ It is possible to run different models with different modelling decisions on the various analysis sets to see how well different models can explain the data, but for simplicity's sake, we're keeping it to one model here!
:::

## Fitting the Model

```{r}
# TODO: Explanation
# TODO: remove example code here and put placeholders for solution
fit_lm_bootstrap <- function(split) {
  linear_reg() %>%
  set_engine("lm") %>%
  fit(
    Sale_Price ~ MS_Zoning + Lot_Area + Overall_Cond + Exter_Cond + Year_Sold, 
      data = analysis(split))
}

# I propose to keep this code, otherwise its really hard. Time is extremely short as is...
#Mapping splits column (containing list of all bootstrap samples (both analysis and assessment sets)) on model above.
boot_ames_models <-
  ames_bootstrap_ex %>% 
  mutate(model = map(splits, fit_lm_bootstrap),
         coef_info = map(model, tidy))
```

## Evaluating the Performance of your Model 

:::danger
Make sure to fit the models on the test-sets first and then use performance metrics :)
:::


```{r eval=FALSE}
model_to_evaluate <- boot_ames_models[1,]$model[[1]]
predictions_lm <- bind_cols(
 select(ames_test_ex, Sale_Price),
 predict(model_to_evaluate, ames_test_ex)
)

# TODO: remove code comments, probably extract chunk to one to evaluate and one to not evaluate
# in order to not kill the knitting

# One or more common performance estimates
# metrics(
#   data = # YOUR CODE HERE
#   , truth = # YOUR CODE HERE
#   , estimate = # YOUR CODE HERE
#   )
# !!! SOLUTION !!!
metrics(data= predictions_lm, truth = Sale_Price, estimate = .pred)

# Calculate the coefficient of determination (the traditional one)
# Who has the highest value?

# Answer: rsq_trad(predictions_lm, truth = Sale_Price, estimate = .pred)
```

---

# Sources

Toropova, A. P., & Toropov, A. A. (2017). The index of ideality of correlation: A criterion of predictability of QSAR models for skin permeability? Science of The Total Environment, 586, 466‚Äì472. https://doi.org/10.1016/j.scitotenv.2017.01.198

