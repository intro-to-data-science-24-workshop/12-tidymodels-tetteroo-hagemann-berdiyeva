---
title: "Modelling with `tidymodels`"
subtitle: "Practice Materials"
author: "Franka Tetteroo, Linus Hagemann & Sofiya Berdiyeva"
date: "28.10.2024"
output: 
    rmdformats::robobook:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    toc_depth: 3
    toc_float: true
    self_contained: false
---

```{=html}
<style>
.h1,h2,h3 {
color:#2f1a61;
}

.subtitle, section.normal {
color:#291854;
}

.title {
color:#cc0065;
}

.nav-pills>li>a{
color: #2f1a61;
}

.nav-pills>li.active>a, .nav-pills>li.active>a:hover, .nav-pills>li.active>a:focus {
color: #fff;
background-color: #2f1a61;
}

.nav-tabs>li>a{
color: #2f1a61;
}

.nav-tabs>li.active>a, .nav-tabs>li.active>a:hover, .nav-tabs>li.active>a:focus {
color: #fff;
background-color: #2f1a61;
}

div.danger {background-color:#c42d2d; color:white; border-radius: 5px; padding: 20px;}

</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

This session is designed to guide you through the entire modelling process to familiarize yourself with some of the power of the packages in `tidymodels`. 

We will cover:

- splitting your data-set into a training and a test set using `rsample`,
- pre-processing your data-set using `recipes`,
- fitting a model using `parsnip`,
- and finally evaluate the performance of the model using `yardstick`.

For those of you attending this session live - stay tuned, as __there will be a price to be won in the end__!

![](./attendance_price.jpg)

::: danger
⚠️ __Attention__

Modelling is an extremely complex topic. Choosing an appropriate model for a specific task requires theoretical deliberation and careful consideration of the data at hand. As was seen in the lecture, hand-picking specific model configurations can rightfully be criticized and requires justification grounded in theory.

All of these are outside of the scope of this workshop - we aim at giving you a glimpse on how to use selected `R` packages. Doing so in a very condensed fashion requires us to take some shortcuts and trade-offs. Please be mindful of this when consulting this material for your future modelling adventures - there's a lot of more to think about!
:::

# Our Dataset

Throughout this exercise, we are working with a specially prepared version of the [`ames`](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt) data-set, which is part of the `modeldata` package. The data-set contains information about houses in the region of Ames in Iowa (US). We prepared a smaller version of the data-set for this unit, in order to reduce the amount of different features (columns) to understand in the data-set and introduce some need for pre-processing.

```{r}
# install.packages("tidymodels")
library(tidymodels)
load("ames_workshop.RData") # executing this line will lead to the `ames_workshop` df becoming available in your environment
ames_workshop
```

As we can see, there are close to 3000 houses in the data-set, with 16 data points for each house.

The following table gives you a quick overview of the available features (in the order in which the columns appear in the data frame). More detailed information can be found [here](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt).

| Name         | Description                                                       |
|--------------|-------------------------------------------------------------------|
| MS_Zoning    | Identifies the general zoning classification of the sale.         |
| Lot_Frontage | Linear feet of street connected to property                       |
| Lot_Area     | Lot size in square feet                                           |
| Utilities    | Type of utilities available                                       |
| Neighborhood | Physical locations within Ames city limits (map available)        |
| Condition_1  | Proximity to various conditions                                   |
| Condition_2  | Proximity to various conditions (if more than one is present)     |
| Overall_Cond | Rates the overall condition of the house                          |
| Exter_Cond   | Evaluates the present condition of the material on the exterior   |
| Foundation   | Type of foundation                                                |
| Heating      | Type of heating                                                   |
| Year_Sold    | Year Sold (YYYY)                                                  |
| Garage_Area  | Size of garage in square feet                                     |
| Mo_Sold      | Month Sold (MM)                                                   |
| Sale_Price   | Sale price \$$                                                    |
| secret_sauce | A special feature just for you! Can this improve our predictions? |

Our goal in this session is to predict the `Sale_Price` of a house based on the other available data-points. For this, we will build a linear model. You can already start to think about which features you think would be most useful for this task! 

# Splitting the Data-Set
We are going to use the package Rsample: in what ways can we split our dataset, what kind of output can we expect and some next steps. 

# Example.
    
1. Loading and transforming the data with recipe


2. Sampling & bootstrapping

## Initial Split
We first split the entire dataset into the training and testing dataset (see slide x). For a split of 75-25% we use .75 as argument.

```{r eval = FALSE}
ames_split <- rsample::initial_split(ames_workshop, prop = .75)
ames_split

```
We can see it partitioned the datset into training (2197 observations) and testing (733 observations). It also gives us the total amount, namely 2930.

## Training and testing
We need one more step to have our training and testing dataset:  

```{r}
ames_train <- rsample::training(ames_split)
ames_test <- rsample::testing(ames_split)
```

Now we're ready to do some advanced splitting of our training dataset!


## Cross validation and V-Fold
This example will focus on v-fold cross validation to further split our training set. Basically, we tell R to split our dataset x amount of times in two groups with a set size (without replacement).

```{r}
ames_folds <- rsample::vfold_cv(ames_train, v = 5)
ames_folds

## to inspect the first fold we do the following:
#to see the amount of analysis vs assessment observations of the first fold:
ames_folds$splits[[1]] %>% analysis() %>% dim()
#to look into the analysis set of the first split:
ames_folds$splits[[1]] %>% analysis() 

```

As you can see, we have not split the training set into 5 sets that consist of an analysis and assessment set. The analysis sets are used to run one or multiple models on, the assessment set acts as a test set to see how well the models explain new data. During research, we can use these various analysis and assessment sets to compare different models and see which one we want to run on our test set. Since we can only run one model on the test set, we need to be confident in the performance of our final model.

!!!Important that the models are run on the analysis sets of the ames_folds tibble :) 
3. Model creation


4. Evaluation (run model on test set (ames_test) and use that for performance estimate :)
```{r eval = FALSE}
# parsnip model ALWAYS returns the results as a data frame 
predictions <- bind_cols(
  select(auto_test, mpg),
  predict(model, auto_test)
)

# And here comes yardstick
# When "truth" is a factor: accuracy and the Kappa
# When "truth" is numeric: rmse, rsq and mae.

# Estimate one or more common performance estimates depending on the class of truth 
metrics(predictions, truth = older, estimate = .pred)

# Regression metrics
reg_metrics <- metric_set(huber_loss, iic, mape, mase)

# The returned function has the same arguments as metrics()
reg_metric(predictions, truth = mpg, estimate = .pred)

```


# Exercise.

1. Loading and transforming the data with recipe


2. Sampling & bootstrapping

2.1 Split the transformed dataset into a training and testing set. Use a proportion of .8. Extract the training and testing dataset from the initial split set. 
```{r eval = FALSE}
#write your code here

#answer: 
ames_split_ex <- rsample::initial_split(ames_workshop, prop = .8)
ames_split_ex

ames_train_ex <- rsample::training(ames_split_ex)
ames_test_ex <- rsample::testing(ames_split_ex)

```

2.2 Bootstrapping:
Bootstrapping is the second method we can employ to split our training set. This time we sample from the training set with replacement until we get the same amount of the training set. The unsampled observations are then put in the assessment (out of the bag) set. For more info, see slide x. 

Use the rsample::bootstraps function to split the dataset 3 times. Note: where vfold_cv uses v = x, bootstraps employs times = x. Extract the analysis sets.
```{r eval = FALSE}
ames_bootstrap_ex <- rsample::bootstraps(ames_train_ex, times = 3)
ames_bootstrap_ex
#this gives us a tibble with two columns: column 1 contains lists: each list then contains an analysis and assessment set. The second column gives us the bootstrap ID, this become more important when working with a large number of bootstrap samples.

#Inspect the first bootstrap sample. How many observations do you expect in the analysis set? 
ames_bootstrap_analysis <- ames_bootstrap_ex$splits[[1]] %>% analysis()
ames_bootstrap_assessment <- ames_bootstrap_ex$splits[[1]] %>% assessment()

```

Note: It is possible to run different models with different modelling decisions on the various analysis sets to see how well different models can explain the data, but for simplicity's sake, we're keeping it to one model here.  

3. Model creation

```{r eval = FALSE}
#For Linus: 
#Just to show what kind of code I used to map over the splits lists to get the model output in the penguin example:

#so basically by putting in analysis(split) in the model, it's telling R to take the analysis set from the bootstrap sample and using model on it.
fit_lm_penguin_bootstrap <- function(split) {
    lm(body_mass_g ~ flipper_length_mm + sex, analysis(split), start = list(k = 1, b = 0))
}

#Mapping splits column (containing list of all bootstrap samples (both analysis and assessment sets)) on model above.
boot_penguin_models <-
  penguin_bootstrap %>% 
  mutate(model = map(splits, fit_lm_penguin_bootstrap),
         coef_info = map(model, tidy))


```


4. Evaluation (make sure to run models on test set first and then use performance metrics :) 

```{r eval = FALSE}
# TO BE CORRECTED!!!

predictions_lm <- bind_cols(
 select(auto_test, mpg),
 predict(linreg_reg_model, auto_test)
)

# One or more common performance estimates
# metrics(data= YOUR CODE, truth = YOUR CODE, estimate = YOUR CODE)

# Calculate symmetric mean absolute percentage error
# Who has the lowest value?

# Answer: smape(predictions_lm, truth = mpg, estimate = .pred)

# OR

# Calculate the coefficient of determination (the traditional one)
# Who has the highest value?

# Answer: rsq_trad(predictions_lm, truth = mpg, estimate = .pred)
```

# Sources

Include Sources Here
