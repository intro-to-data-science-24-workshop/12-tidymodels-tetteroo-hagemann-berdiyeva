---
title: "Introduction to Data Science - Workshop 2024"
subtitle: "Session 12: Modelling with `tidymodels`"
author: "Franka Tetteroo, Linus Hagemann & Sofiya Berdiyeva"
output:
  xaringan::moon_reader:
    includes:
    css: [default, metropolis, metropolis-fonts] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
      hash: true
---


```{css, echo=FALSE} 
@media print { # print out incremental slides; see https://stackoverflow.com/questions/56373198/get-xaringan-incremental-animations-to-print-to-pdf/56374619#56374619
  .has-continuation {
    display: block !important;
  }
}
```


# **Rsample** package
.pull-left[
Useful to split your data into testing and training subsets:

Why do we care? 
- Having multiple splits allows us to construct and compare different models with different (combinations of) parameters.
- The final model is only used on the test set once, so we want to make sure that the final model is the best way of explaining the data.
  - If we run multiple models on the test set, we start to optimise our model to the test set instead of focusing on the model's general ability to explain the dataset. 
]

.pull-right[
<div align="center">
<img src="pics/Datasplittingscheme.png" height=300>
</div>
]
---
# **Different ways of splitting your dataset**


.pull-left[
- Initial split (training vs testing) --> initial_split(strata = desired column, prop = 0.x)
- V-fold
- Bootstraps
- Rolling forecast
]

.pull-right[
<div align="center">
<img src="pics/One_Does_Not_Simply_Split.png" height=300>
</div>
]
---
# V-fold



<div align="center">
<img src="pics/V-fold.png" height=300>
</div>

---
# Bootstrapping

.pull-left[
- Analysis sets are sampled from training dataset **with** replacement.
  - Has the same size as the training dataset
- Analysis vs Assessment (Out-of-the bag) split
  - Why does the assessment set differ in size?
- Function: bootstraps(dataset, times = x).

- (insert picture from practice example to show output of a bootstrap)
]


.pull-right[
<div align="center">
<img src="pics/Bootstrap.png" height=300>
</div>
]
---
# Rolling Forecasting Origing Resampling
.pull-left[
- If our dataset has a temporal component, simply random sampling can disrupt the model in estimating patterns.
  - Solution: fit_resamples(formula, resamples, ...)
- We set the size of the analysis and assessment sets. The first sample follows this division, the second iteration takes the same data size but shifts by a specified number. 
- Alternatively we don't have to shift by one day but also by one week or more. 
]

.pull-right[
<div align="center">
<img src="pics/RollingForecasting.png" height=300>
</div>
]
---
# Preprocessing and Feature Engineering

- Data needs to adhere to some model-specific rules and assumptions

> __Feature Engineering__
> The process of transforming raw data into features well suited-for modelling.

- What does this entail?
    - removing NA's -> how to deal with them? Throw data away? Impute with mean/median/...?
    - Transform numeric values (`log`, `sqrt`,...)
    - Rescale numeric values
    - deal with nominal/string values
    
---
# Pre-processing with recipes

- While all of this can be coded by hand, `recipes` make our life's easier by providing the necessary operations in a standardized way
- familiar to `tidyverse` users and compatible with `dplyr` 
```r
rec <- recipe(predictor ~ ., data = training_data) %>%
      step_corr(all_predictors()) %>% # remove features with high correlation between them
      step_string2factor(all_string_predictors()) %>%
      step_impute_mean(all_numeric_predictors()) %>%
      step_mutate(<dplyr call>)
```
--
- can be reused easily, as we can rely on roles and not specific variable names
--
<br>
- Use
    - `prep()` to calculate pre-processing parameters (e.g., mean to impute)
    - `bake()` to apply the pre-processing steps to the actual data
---
# Modelling in R

- Many different models to choose from
    - Type of model to use is an important decision, which needs to grounded __in theory__
- For each type of model, multiple implementations are available
- Consider these examples for linear regression models:
---
# Modelling in R

- `stat::lm()`
```r
lm(formula, data, [...], ...)
```
--

- `glmnet::glm()`
```r
glmnet(x, y, [...], standardize = TRUE, intercept = TRUE, ...)
```
--

- `sparklyr::ml_linear_regression()`
```r
ml_linear_regression(x, formula = NULL, fit_intercept = TRUE, [...], standardization = TRUE, [...], ...)
```
---
# Modelling in R
.pull-left[
- Syntax differs
    - in argument order
    - in argument names
    - (in output formatting/reporting -> __later__)
    
<img src="pics/so_many_syntaxes.jpg" height=300>
]
--
.pull-right[
- Parsnip to the rescue!

<img src="pics/parsnip_logo.png" height=300>
]
---
# parsnip

- package that aims to unify signatures of different models in `R`
- one familiar approach to **many different types of models**

```
  linear_reg() %>% 
  set_engine("lm") %>% 
  fit(dependent ~ pred_one + pred_two, data = data)
```
--
<br>
- `linear_reg()`, `decision_tree()`, `logistic_reg()`, ...
- `set_engine` to specify which implementation to use
- unified way of fitting a model to a dataset: `fit()`
- unified way of predicting the outcome on a dataset: `predict(fitted_model, data)`

---
# **yardstick** package

Contains tools for computing statistical performance metrics

- Consistent Syntax 
- Wide Range of Metrics (43)
- Grouped calculations (e.g. `demographic_parity()`)
- Customization (e.g. `new-metric()` see more on [custom performance metrics](https://www.tidymodels.org/learn/develop/metrics/))

`metrics(data, truth, estimate, ..., na_rm = TRUE, options = list())`

[Manual](https://cran.r-project.org/web/packages/yardstick/yardstick.pdf)

<div align="center">
<img src="pics/Metrics_meme.png" height=300>
</div>


---
# Classification metrics

Measuring the accuracy in prediction of categories

| Function         | Name                              | Interpretation                   |
|------------------|-----------------------------------|----------------------------------|
| accuracy()       | Accuracy                          | 0 < x < 1, the higher the better |
| f_meas()         | F-measure                         | 0 < x < 1, the higher the better |
| kap()            | Cohen's Kappa                     | -1 < x < 1, the closer to 1 the better |
| mcc()            | Matthews Correlation Coefficient   | -1 < x < 1, the closer to 1 the better |
| npv()            | Negative Predictive Value         | 0 < x < 1, the higher the better |
| ppv()            | Positive Predictive Value         | 0 < x < 1, the higher the better |
| precision()          | Precision                   | 0 < x < 1, the higher the better   |
| recall()             | Recall                      | 0 < x < 1, the higher the better   |
| spec()               | Specificity                 | 0 < x < 1, the higher the better   |
| roc_auc()            | Area Under ROC Curve         | 0.5 < x < 1, the higher the better |
| pr_auc()             | Area Under PR Curve          | 0 < x < 1, the higher the better   |


---
# Regression metrics

Measuring the accuracy in prediction of continuous numeric values

| Function              | Name                         | Interpretation                     |
|-----------------------|------------------------------|------------------------------------|
| ccc()                 | Concordance Correlation Coefficient | -1 < x < 1, the closer to 1 the better |
| iic()                 | Index of Ideality of Correlation | 0 < x < 1, the higher the better    |
| **mae()**                 | Mean Absolute Error           | 0 < x < $\infty$, the lower the better     |
| mape()                | Mean Absolute Percentage Error | 0 < x < $\infty$, the lower the better    |
| msd()                 | Mean Squared Deviation        | 0 < x < $\infty$, the lower the better     |
| poisson_log_loss()    | Poisson Log Loss              | 0 < x < $\infty$, the lower the better     |
| **rmse()**                | Root Mean Squared Error       | 0 < x < $\infty$, the lower the better     |
| rpd()                 | Relative Percent Difference    | 0 < x < 1, the lower the better     |
| **rsq_trad()**            | Traditional R-squared         | 0 < x < 1, the closer to 1 the better |
| smape()               | Symmetric Mean Absolute Percentage Error | 0 < x < 1, the lower the better |

---
# Further Ressources

- ðŸŒ Website of the [`tidymodels`](https://www.tidymodels.org/) package, including documentation and guides
    - `tidymodels` consists of many more packages than we could cover in this session!
- ðŸ“– [Tidy Modelling with R](https://www.tmwr.org/)
- ðŸ“º [Get started with tidymodels and classification of penguin data by Julia Silge](Get started with tidymodels and classification of penguin data)
- ðŸ“º [tidymodels: Adventures in Rewriting a Modeling Pipeline - posit::conf(2023)](https://www.youtube.com/watch?v=R7XNqcCZnLg)
